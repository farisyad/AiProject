{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkHlltfYpnDo",
        "outputId": "5fd91fa4-2a38-4e3a-9ce1-9af19b9c8fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install icrawler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZmY47J8WwSr",
        "outputId": "4c76fdba-e447-4ca9-ce6c-54259cecd4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.9-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from icrawler) (4.12.3)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from icrawler) (5.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from icrawler) (11.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from icrawler) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->icrawler) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->icrawler) (2024.12.14)\n",
            "Downloading icrawler-0.6.9-py3-none-any.whl (34 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW42ueJXAFCy",
        "outputId": "fc50a01b-43b5-4eaa-945f-ef9b68e1678e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTNtRzawOcJx",
        "outputId": "52b4605e-abd5-4a7b-b010-67235dad0d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Du8dz85WCvo"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Your Unsplash Access Key\n",
        "ACCESS_KEY = \"H1IVJhiehu6CL6hTwubD6biPzyvCz8F2u8qW-iadqfI\"\n",
        "\n",
        "# Function to resize and compress images\n",
        "def resize_and_compress(image_path, output_path, max_size=(800, 800), quality=85):\n",
        "    \"\"\"\n",
        "    Resize and compress an image.\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "        output_path (str): Path to save the compressed image.\n",
        "        max_size (tuple): Maximum width and height for resizing.\n",
        "        quality (int): Compression quality (1-100, higher is better quality).\n",
        "    \"\"\"\n",
        "    with Image.open(image_path) as img:\n",
        "        img.thumbnail(max_size)  # Resize while maintaining aspect ratio\n",
        "        img.save(output_path, \"JPEG\", quality=quality)  # Save as JPEG with compression\n",
        "\n",
        "# Function to download images from Unsplash\n",
        "def download_images_unsplash(query, output_dir, num_images=100):\n",
        "    \"\"\"\n",
        "    Download images from Unsplash API.\n",
        "    Args:\n",
        "        query (str): Search query for the images.\n",
        "        output_dir (str): Directory to save the images.\n",
        "        num_images (int): Total number of images to download.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
        "    url = \"https://api.unsplash.com/search/photos\"\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Client-ID {ACCESS_KEY}\"\n",
        "    }\n",
        "\n",
        "    total_downloaded = 0\n",
        "    page = 1\n",
        "\n",
        "    while total_downloaded < num_images:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"page\": page,\n",
        "            \"per_page\": 30  # Unsplash returns a max of 30 images per page\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "\n",
        "        if response.status_code == 403:  # Rate Limit Exceeded\n",
        "            print(\"Rate limit reached. Waiting for reset...\")\n",
        "            reset_time = int(response.headers.get(\"X-Ratelimit-Reset\", 60))  # Default 60 seconds\n",
        "            time.sleep(reset_time)  # Wait for rate limit to reset\n",
        "            continue\n",
        "        elif response.status_code != 200:\n",
        "            print(f\"Failed to fetch images: {response.status_code}\")\n",
        "            print(\"Response:\", response.text)\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            data = response.json()  # Attempt to parse the response as JSON\n",
        "        except requests.exceptions.JSONDecodeError:\n",
        "            print(\"Failed to decode JSON from the response.\")\n",
        "            print(\"Response content:\", response.text)\n",
        "            break\n",
        "\n",
        "        images = data.get(\"results\", [])\n",
        "\n",
        "        if not images:\n",
        "            print(\"No more images found.\")\n",
        "            break\n",
        "\n",
        "        for image in images:\n",
        "            if total_downloaded >= num_images:\n",
        "                break\n",
        "\n",
        "            image_url = image[\"urls\"][\"full\"]  # Full-resolution image URL\n",
        "            image_id = image[\"id\"]  # Unique image ID\n",
        "\n",
        "            try:\n",
        "                img_response = requests.get(image_url, stream=True)\n",
        "\n",
        "                if img_response.status_code == 200:\n",
        "                    raw_image_path = os.path.join(output_dir, f\"{image_id}_raw.jpg\")\n",
        "                    compressed_image_path = os.path.join(output_dir, f\"{image_id}.jpg\")\n",
        "\n",
        "                    # Save the raw image temporarily\n",
        "                    with open(raw_image_path, \"wb\") as file:\n",
        "                        file.write(img_response.content)\n",
        "\n",
        "                    # Resize and compress the raw image\n",
        "                    resize_and_compress(raw_image_path, compressed_image_path)\n",
        "\n",
        "                    # Remove the raw image to save space\n",
        "                    os.remove(raw_image_path)\n",
        "\n",
        "                    total_downloaded += 1\n",
        "                    print(f\"Downloaded and compressed {total_downloaded}/{num_images}: {compressed_image_path}\")\n",
        "                else:\n",
        "                    print(f\"Failed to download image {image_id}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading image {image_id}: {e}\")\n",
        "\n",
        "        page += 1  # Move to the next page\n",
        "        time.sleep(1)  # Add delay between pages to avoid hitting rate limits\n",
        "\n",
        "# Dataset with species and subspecies\n",
        "dataset = {\n",
        "    #\"Bald eagle\": [\"Bald eagle\"],\n",
        "    \"Harpy Eagle\": [\"Harpy Eagle\"],\n",
        "    #\"Golden eagle\": [\"Golden Eagle\"],\n",
        "    #\"Polar Bear\": [\"Polar Bear\"],\n",
        "    #\"Grizzly bear\": [\"Grizzly Bear\"],\n",
        "    #\"Siberian Tiger\": [\"Siberian Tiger\"],\n",
        "    #\"Sumatran Tiger\": [\"Sumatran Tiger\"],\n",
        "    #\"African elephant\": [\"African Elephant\"],\n",
        "    #\"Asian elephant\": [\"Asian Elephant\"],\n",
        "\n",
        "\n",
        "    # Add more species and subspecies here\n",
        "}\n",
        "\n",
        "# Base directory for saving images\n",
        "output_dir_base = \"/content/drive/MyDrive/Unsplash_Images\"\n",
        "\n",
        "# Number of images per query\n",
        "num_images = 700  # Change this to the total number of images you want per query\n",
        "\n",
        "# Download images for each species and subspecies\n",
        "for species, subspecies_list in dataset.items():\n",
        "    for subspecies in subspecies_list:\n",
        "        output_dir = os.path.join(output_dir_base, species, subspecies.replace(\" \", \"_\"))\n",
        "        download_images_unsplash(subspecies, output_dir, num_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_dataset(base_dir):\n",
        "    \"\"\"\n",
        "    Count the number of images for each species and subspecies in the dataset directory.\n",
        "\n",
        "    Args:\n",
        "        base_dir (str): Base directory containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        dict: A nested dictionary with species as keys and subspecies counts as values.\n",
        "    \"\"\"\n",
        "    image_counts = {}\n",
        "\n",
        "    for species in os.listdir(base_dir):\n",
        "        species_dir = os.path.join(base_dir, species)\n",
        "        if not os.path.isdir(species_dir):\n",
        "            continue\n",
        "\n",
        "        species_counts = {}\n",
        "\n",
        "        for subspecies in os.listdir(species_dir):\n",
        "            subspecies_dir = os.path.join(species_dir, subspecies)\n",
        "            if not os.path.isdir(subspecies_dir):\n",
        "                continue\n",
        "\n",
        "            # Count .jpg files in the subspecies directory\n",
        "            image_count = sum(\n",
        "                1 for file in os.listdir(subspecies_dir)\n",
        "                if file.lower().endswith((\".jpg\", \".jpeg\"))\n",
        "            )\n",
        "            species_counts[subspecies] = image_count\n",
        "\n",
        "        image_counts[species] = species_counts\n",
        "\n",
        "    return image_counts\n",
        "\n",
        "# Base directory for the dataset\n",
        "output_dir_base = \"/content/drive/MyDrive/Unsplash_Images\"\n",
        "\n",
        "# Count images in the dataset\n",
        "image_counts = count_images_in_dataset(output_dir_base)\n",
        "\n",
        "# Display the counts\n",
        "for species, subspecies_counts in image_counts.items():\n",
        "    print(f\"\\nSpecies: {species}\")\n",
        "    for subspecies, count in subspecies_counts.items():\n",
        "        print(f\"  Subspecies: {subspecies.replace('_', ' ')} - {count} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_UnUavH6cbE",
        "outputId": "1249a203-ef6c-4dd6-8b35-0a655b422a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Species: Polar Bear\n",
            "  Subspecies: Polar Bear - 399 images\n",
            "\n",
            "Species: Siberian Tiger\n",
            "  Subspecies: Siberian Tiger - 828 images\n",
            "\n",
            "Species: Golden eagle\n",
            "  Subspecies: Golden Eagle - 119 images\n",
            "\n",
            "Species: Grizzly bear\n",
            "  Subspecies: Grizzly Bear - 350 images\n",
            "\n",
            "Species: Sumatran Tiger\n",
            "  Subspecies: Sumatran Tiger - 682 images\n",
            "\n",
            "Species: African elephant\n",
            "  Subspecies: African Elephant - 607 images\n",
            "\n",
            "Species: Asian elephant\n",
            "  Subspecies: Asian Elephant - 546 images\n",
            "\n",
            "Species: Bald eagle\n",
            "  Subspecies: Bald eagle - 627 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset(input_dir, output_dir, train_percent=0.7, val_percent=0.15, test_percent=0.15):\n",
        "    \"\"\"\n",
        "    Split dataset into training, validation, and test subsets.\n",
        "\n",
        "    Args:\n",
        "        input_dir (str): The directory containing the original dataset.\n",
        "        output_dir (str): The directory where the split dataset will be saved.\n",
        "        train_percent (float): The percentage of images to be used for training.\n",
        "        val_percent (float): The percentage of images to be used for validation.\n",
        "        test_percent (float): The percentage of images to be used for testing.\n",
        "    \"\"\"\n",
        "    # Ensure the output directories exist\n",
        "    train_dir = os.path.join(output_dir, 'train')\n",
        "    val_dir = os.path.join(output_dir, 'val')\n",
        "    test_dir = os.path.join(output_dir, 'test')\n",
        "\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through each species directory\n",
        "    for species in os.listdir(input_dir):\n",
        "        species_dir = os.path.join(input_dir, species)\n",
        "\n",
        "        if not os.path.isdir(species_dir):\n",
        "            continue\n",
        "\n",
        "        # Create corresponding directories for train, val, and test sets\n",
        "        os.makedirs(os.path.join(train_dir, species), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_dir, species), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_dir, species), exist_ok=True)\n",
        "\n",
        "        # Iterate through each subspecies directory under species\n",
        "        for subspecies in os.listdir(species_dir):\n",
        "            subspecies_dir = os.path.join(species_dir, subspecies)\n",
        "\n",
        "            if not os.path.isdir(subspecies_dir):\n",
        "                continue\n",
        "\n",
        "            # List all image files in the subspecies directory\n",
        "            images = [f for f in os.listdir(subspecies_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            random.shuffle(images)  # Shuffle the images randomly\n",
        "\n",
        "            num_images = len(images)\n",
        "            num_train = int(train_percent * num_images)\n",
        "            num_val = int(val_percent * num_images)\n",
        "            num_test = num_images - num_train - num_val\n",
        "\n",
        "            # Split images into train, val, and test\n",
        "            train_images = images[:num_train]\n",
        "            val_images = images[num_train:num_train+num_val]\n",
        "            test_images = images[num_train+num_val:]\n",
        "\n",
        "            # Copy the images into corresponding directories\n",
        "            for img in train_images:\n",
        "                shutil.copy(os.path.join(subspecies_dir, img), os.path.join(train_dir, species, img))\n",
        "            for img in val_images:\n",
        "                shutil.copy(os.path.join(subspecies_dir, img), os.path.join(val_dir, species, img))\n",
        "            for img in test_images:\n",
        "                shutil.copy(os.path.join(subspecies_dir, img), os.path.join(test_dir, species, img))\n",
        "\n",
        "    print(\"Dataset splitting complete!\")\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "input_dir = \"/content/drive/MyDrive/Unsplash_Images\"  # Directory where your dataset is stored\n",
        "output_dir = \"/content/drive/MyDrive/Unsplash_Images_Split\"  # Directory where the split dataset will be saved\n",
        "\n",
        "# Split the dataset into 70% train, 15% validation, and 15% test\n",
        "split_dataset(input_dir, output_dir, train_percent=0.7, val_percent=0.15, test_percent=0.15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmy2l6J98UhC",
        "outputId": "77a35f20-3d30-4b4e-de6c-8877dabb56ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset splitting complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below deletes the directory (files and subdirectories) permanently."
      ],
      "metadata": {
        "id": "f3iDkSe-RcRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/drive/MyDrive/Unsplash_Images/Harpy Eagle')"
      ],
      "metadata": {
        "id": "YXFa2ZGipzb2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}